---
title: "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming"
collection: publications
date: 2024-05-15
permalink: /publication/Bilingual
venue: "Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)"
paperurl: "https://arxiv.org/abs/2405.09508"
citation: "Zhang, D., Xiao, B., Gao, C., Youm, S., & Dorr, B. (2024). Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming. In Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)."
excerpt: "This paper evaluates RNN and Transformer models for replicating cross-language structural priming in Chinese-English, showing Transformers outperform RNNs in producing primed sentence structures."
---

This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a cognitive phenomenon where prior exposure to a sentence structure increases the likelihood of selecting a similar one later. Focusing on Chinese-English priming, the authors found that Transformer models outperform RNNs in generating primed structures, with accuracy ranging from 25.84% to 33%. These findings challenge the idea that human sentence processing is purely recurrent and suggest a cue-based mechanism better reflected by Transformer behavior.

[Download paper here](https://arxiv.org/abs/2405.09508)
